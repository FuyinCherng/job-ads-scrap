#------ read features extracted from train set, using your python script
db=read.csv('/Users/fuyincherng/Documents/EPFLCourse/DigitalEducation/R/Project/OutputTable_train.csv', stringsAsFactors = F)
#------ sort submissions
db=db[order(db$UserID,db$ProblemID,db$SubmissionNumber),]
#--- replace NA values with 0
db[is.na(db)]=0
#----- remove first submissions
db= filter(db,SubmissionNumber>0)
#---- remove cases when there is no video or forum activity between two submissions
db$NVideoAndForum= db$NVideoEvents+db$NForumEvents
db= filter(db,NVideoAndForum>0)
#----- make a catgorical vribale, indicating if grade improved
db$improved = factor(ifelse(db$GradeDiff>0 ,'Yes', 'No' ))
table(db$improved)
write.csv(db, file = "/Users/fuyincherng/Documents/EPFLCourse/DigitalEducation/R/Project/db_tagged.csv")
#------ read features extracted from train set, using your python script
db=read.csv('/Users/fuyincherng/Documents/EPFLCourse/DigitalEducation/R/Project/OutputTable_train.csv', stringsAsFactors = F)
#------ sort submissions
db=db[order(db$UserID,db$ProblemID,db$SubmissionNumber),]
#--- replace NA values with 0
db[is.na(db)]=0
#----- remove first submissions
db= filter(db,SubmissionNumber>0)
#---- remove cases when there is no video or forum activity between two submissions
db$NVideoAndForum= db$NVideoEvents+db$NForumEvents
db= filter(db,NVideoAndForum>0)
#----- make a catgorical vribale, indicating if grade improved
db$improved = factor(ifelse(db$GradeDiff>0 ,'Yes', 'No' ))
table(db$improved)
write.csv(db, file = "/Users/fuyincherng/Documents/EPFLCourse/DigitalEducation/R/Project/db_tagged.csv")
# ----- (Optional) split your training data into train and test set. Use train set to build your classifier and try it on test data to check generalizability.
set.seed(1234)
tr.index= sample(nrow(db), nrow(db)*0.9)
db.train= db[tr.index,]
db.test = db[-tr.index,]
dim(db.train)
dim(db.test)
#----- train classifier to predict 'improved' status
#----- Try different methods, model parameters, feature sets and find the best classifier
#----- Use AUC as model evaluation metric
library(caret)
paramGrid <- expand.grid(mtry = c(1,2,3))
fs=c('TimeSinceLast','SubmissionNumber',
'NumberOfThreadViews', 'DurationOfVideoActivity',
'NumberOfLaunch','NForumAndVideo',
'DurationOfForumActivity','AverageVideoTimeDiffs','NumberOfVideoLoad',
'NumberOfVideoPlay','NumberOfVideoDownload',
'NumberOfVideoPauseSeek','NumberOfVideoSpeedChange',
'NumberOfPostsEvent','NumberOfSubscribe','NumberOfComments','Postlength','FrequentSubmit', 'earlysubmit')
ctrl= trainControl(method = 'cv', summaryFunction=twoClassSummary ,classProbs = TRUE)
model=train(x=db.train[,fs],
y=db.train$improved,
method = "rf",
metric="ROC",
trControl = ctrl,
tuneGrid = paramGrid,
preProc = c("center", "scale"))
print(model);   plot(model)
# estimate variable importance
importance <- varImp(model, scale=FALSE)
# summarize importance
print(importance)
# plot importance
plot(importance)
paramGrid <- expand.grid(mtry = c(1,2,3))
fs=c('TimeSinceLast','SubmissionNumber',
'NumberOfThreadViews', 'DurationOfVideoActivity',
'NumberOfLaunch','NForumAndVideo',
'DurationOfForumActivity','AverageVideoTimeDiffs','NumberOfVideoLoad',
'NumberOfVideoPlay','NumberOfVideoDownload',
'NumberOfVideoPauseSeek','NumberOfVideoSpeedChange',
'NumberOfPostsEvent','NumberOfSubscribe','NumberOfComments','Postlength','FrequentSubmit', 'earlysubmit','UserID', 'ProblemID')
ctrl= trainControl(method = 'cv', summaryFunction=twoClassSummary ,classProbs = TRUE)
model=train(x=db.train[,fs],
y=db.train$improved,
method = "rf",
metric="ROC",
trControl = ctrl,
tuneGrid = paramGrid,
preProc = c("center", "scale"))
print(model);   plot(model)
#------ read features extracted from train set, using your python script
db=read.csv('OutputTable_train.csv', stringsAsFactors = F)
#------ sort submissions
db=db[order(db$UserID,db$ProblemID,db$SubmissionNumber),]
#--- replace NA values with 0
db[is.na(db)]=0
#----- remove first submissions
db= filter(db,SubmissionNumber>0)
#---- remove cases when there is no video or forum activity between two submissions
db$NVideoAndForum= db$NVideoEvents+db$NForumEvents
db= filter(db,NVideoAndForum>0)
#----- make a catgorical vribale, indicating if grade improved
db$improved = factor(ifelse(db$GradeDiff>0 ,'Yes', 'No' ))
table(db$improved)
write.csv(db, file = "db_tagged.csv")
db=read.csv('OutputTable_train.csv', stringsAsFactors = F)
#------ sort submissions
db=db[order(db$UserID,db$ProblemID,db$SubmissionNumber),]
#--- replace NA values with 0
db[is.na(db)]=0
#----- remove first submissions
db= filter(db,SubmissionNumber>0)
#---- remove cases when there is no video or forum activity between two submissions
db$NVideoAndForum= db$NVideoEvents+db$NForumEvents
db= filter(db,NVideoAndForum>0)
#----- make a catgorical vribale, indicating if grade improved
db$improved = factor(ifelse(db$GradeDiff>0 ,'Yes', 'No' ))
table(db$improved)
write.csv(db, file = "db_tagged_new.csv")
db=read.csv('OutputTable_train.csv', stringsAsFactors = F)
#------ sort submissions
db=db[order(db$UserID,db$ProblemID,db$SubmissionNumber),]
#--- replace NA values with 0
db[is.na(db)]=0
#----- remove first submissions
db= filter(db,SubmissionNumber>0)
#---- remove cases when there is no video or forum activity between two submissions
db$NVideoAndForum= db$NVideoEvents+db$NForumEvents
db= filter(db,NVideoAndForum>0)
#----- make a catgorical vribale, indicating if grade improved
db$improved = factor(ifelse(db$GradeDiff>0 ,'Yes', 'No' ))
table(db$improved)
write.csv(db, file = "/Users/fuyincherng/Documents/EPFLCourse/DigitalEducation/R/cs411Project/CS411Project/db_tagged_new.csv")
View(db)
View(db)
#------ read features extracted from train set, using your python script
db=read.csv('/Users/fuyincherng/Documents/EPFLCourse/DigitalEducation/R/cs411Project/CS411Project/OutputTable_train.csv', stringsAsFactors = F)
#------ sort submissions
db=db[order(db$UserID,db$ProblemID,db$SubmissionNumber),]
#--- replace NA values with 0
db[is.na(db)]=0
#----- remove first submissions
db= filter(db,SubmissionNumber>0)
#---- remove cases when there is no video or forum activity between two submissions
db$NVideoAndForum= db$NVideoEvents+db$NForumEvents
db= filter(db,NVideoAndForum>0)
#----- make a catgorical vribale, indicating if grade improved
db$improved = factor(ifelse(db$GradeDiff>0 ,'Yes', 'No' ))
table(db$improved)
write.csv(db, file = "/Users/fuyincherng/Documents/EPFLCourse/DigitalEducation/R/cs411Project/CS411Project/db_tagged_new.csv")
db=read.csv('/Users/fuyincherng/Documents/EPFLCourse/DigitalEducation/R/cs411Project/CS411Project/OutputTable_train.csv', stringsAsFactors = F)
#------ sort submissions
db=db[order(db$UserID,db$ProblemID,db$SubmissionNumber),]
#--- replace NA values with 0
db[is.na(db)]=0
#----- remove first submissions
db= filter(db,SubmissionNumber>0)
#---- remove cases when there is no video or forum activity between two submissions
db$NVideoAndForum= db$NVideoEvents+db$NForumEvents
db= filter(db,NVideoAndForum>0)
#----- make a catgorical vribale, indicating if grade improved
db$improved = factor(ifelse(db$GradeDiff>0 ,'Yes', 'No' ))
table(db$improved)
write.csv(db, file = "/Users/fuyincherng/Documents/EPFLCourse/DigitalEducation/R/cs411Project/CS411Project/db_tagged_new.csv")
db$TimediffVideoSubmit = factor(abs(db$TimeStamp - db$VideoLastTimeStamps))
db$TimediffForumSubmit = factor(abs(db$TimeStamp - db$FTLastTimeStamps))
db=read.csv('/Users/fuyincherng/Documents/EPFLCourse/DigitalEducation/R/cs411Project/CS411Project/OutputTable_train.csv', stringsAsFactors = F)
#------ sort submissions
db=db[order(db$UserID,db$ProblemID,db$SubmissionNumber),]
#--- replace NA values with 0
db[is.na(db)]=0
#----- remove first submissions
db= filter(db,SubmissionNumber>0)
#---- remove cases when there is no video or forum activity between two submissions
db$NVideoAndForum= db$NVideoEvents+db$NForumEvents
db= filter(db,NVideoAndForum>0)
#---- add new factor: submission timestamp - videolasttime and FTlasttime
db$TimediffVideoSubmit = factor(abs(db$TimeStamp - db$VideoLastTimeStamps))
db$TimediffForumSubmit = factor(abs(db$TimeStamp - db$FTLastTimeStamps))
#---- output tagged file of db
write.csv(db, file = "db_tagged.csv")
db=read.csv('/Users/fuyincherng/Documents/EPFLCourse/DigitalEducation/R/cs411Project/CS411Project/OutputTable_train.csv', stringsAsFactors = F)
#------ sort submissions
db=db[order(db$UserID,db$ProblemID,db$SubmissionNumber),]
#--- replace NA values with 0
db[is.na(db)]=0
#----- remove first submissions
db= filter(db,SubmissionNumber>0)
#---- remove cases when there is no video or forum activity between two submissions
db$NVideoAndForum= db$NVideoEvents+db$NForumEvents
db= filter(db,NVideoAndForum>0)
#---- add new factor: submission timestamp - videolasttime and FTlasttime
db$TimediffVideoSubmit = factor(abs(db$TimeStamp - db$VideoLastTimeStamps))
db$TimediffForumSubmit = factor(abs(db$TimeStamp - db$FTLastTimeStamps))
#---- output tagged file of db
write.csv(db, file = "/Users/fuyincherng/Documents/EPFLCourse/DigitalEducation/R/cs411Project/CS411Project/db_tagged.csv")
db=read.csv('/Users/fuyincherng/Documents/EPFLCourse/DigitalEducation/R/cs411Project/CS411Project/OutputTable_train.csv', stringsAsFactors = F)
#------ sort submissions
db=db[order(db$UserID,db$ProblemID,db$SubmissionNumber),]
#--- replace NA values with 0
db[is.na(db)]=0
#----- remove first submissions
db= filter(db,db$SubmissionNumber>0)
#---- remove cases when there is no video or forum activity between two submissions
db$NVideoAndForum= db$NVideoEvents+db$NForumEvents
db= filter(db,db$NVideoAndForum>0)
#---- add new factor: submission timestamp - videolasttime and FTlasttime
db$TimediffVideoSubmit = factor(abs(db$TimeStamp - db$VideoLastTimeStamps))
db$TimediffForumSubmit = factor(abs(db$TimeStamp - db$FTLastTimeStamps))
#---- output tagged file of db
write.csv(db, file = "/Users/fuyincherng/Documents/EPFLCourse/DigitalEducation/R/cs411Project/CS411Project/db_tagged.csv")
db$NVideoAndForum= db$NVideoEvents+db$NForumEvents
db= filter(db,NVideoAndForum>0)
db=read.csv('OutputTable.csv', stringsAsFactors = F)
?confusionMatrix
library(caret)
?confusionMatrix
load("/Users/fuyincherng/Documents/Dataset/LakeMacquarieCityJob/face19d3-7c87-4423-a44f-1bcfc372be80.rdf")
load(/Users/fuyincherng/Documents/Dataset/LakeMacquarieCityJob/face19d3-7c87-4423-a44f-1bcfc372be80.rdf)
readRDS("/Users/fuyincherng/Documents/Dataset/LakeMacquarieCityJob/face19d3-7c87-4423-a44f-1bcfc372be80.rdf")
read.table("/Users/fuyincherng/Documents/Dataset/LakeMacquarieCityJob/face19d3-7c87-4423-a44f-1bcfc372be80.rdf")
read.csv("/Users/fuyincherng/Documents/Dataset/KaggleJobSalaryPrediction/Train_rev1.csv")
# fit the stationary series to arima model and make prediction
# input: stationary series
library(forecast)
library(tseries)
#creating time series
y <- read.csv('/Users/fuyincherng/Documents/EPFLCourse/2017 spring/PerformanceEvaluation/hw/hw4/workingDay_Energy.csv',header= TRUE)
# save a numeric vector containing 72 monthly observations
# from Jan 2009 to Dec 2014 as a time series object
myts <- ts(y)
#, start=c(2009, 1), end=c(2014, 12), frequency=12)
# plot series
# weekly seasonality, s = 24*7 = 168;
plot(myts)
# make difference
lag24 <- diff(myts, lag=24, differences = 1)
plot(lag24)
lag24_168 <- diff(lag24, lag=168, differences = 1)
plot(lag24_168)
lag24_168_24 <- diff(lag24_168, lag=24, differences = 1)
plot(lag24_168_24)
install.packages('	tseries')
plot(log(lag24_168))
plot(log(lag24_168+1))
summary(lag24_168)
plot(lag24_168)
adf.test(lag24_168);
acf(lag24_168)
pacf(lag24_168)
fit_2 <- arima(myts,c(1,0,0))
accuracy(fit_2)
?accuracy
accuracy(fit_2)
fit_3 <- arima(myts,c(2,0,0))
accuracy(fit_3)
AIC(fit_2)
AIC(fit_3)
predic <- forecast(fit_2, 168)
plot(forecast(fit_2, 168))
fit_2 <- arima(lag24_168,c(1,0,0))
accuracy(fit_2)
fit_3 <- arima(lag24_168,c(2,0,0))
accuracy(fit_3)
AIC(fit_2)
AIC(fit_3)
predic <- forecast(fit_2, 168)
plot(forecast(fit_2, 168))
predic <- forecast(fit_2, n.ahead = 5)
plot(forecast(fit_2, n.ahead = 5))
?forecast
?predict
result <- predict(fit_2, n.head=6)
plot(result)
print(result)
predic$mean
predic$level
predic$lower
predic <- forecast(fit_2, 5)
result <- predic$mean
plot(result)
predic <- forecast(fit_2, 168)
result <- predic$mean
plot(result)
y[1:5]
y[1:5,]
length(y)
size(y)
View(y)
View(y)
col(y)
nrow(y)
myts <- ts(y[1:nrow(y)-28,])
ts_test <- y[nrow(y)-27:nrow(y)-28,]
myts <- ts(y[1:(nrow(y)-28),])
ts_test <- y[(nrow(y)-27):(nrow(y)-28),]
plot(myts)
plot(ts_test)
ts_test <- y[(nrow(y)-27):nrow(y),]
plot(ts_test)
lag24 <- diff(myts, lag=24, differences = 1)
plot(lag24)
lag24_168 <- diff(lag24, lag=168, differences = 1)
plot(lag24_168)
adf.test(myts);
adf.test(lag24_168);
acf(myts)
pacf(myts)
acf(myts)
acf(lag24_168)
pacf(lag24_168)
pacf(myts)
pacf(lag24_168)
fit_3 <- arima(lag24_168,c(2,0,0))
accuracy(fit_3)
lag24_168_test_1 <- diff(ts_test, lag=24, differences = 1)
lag24_168_test <- diff(lag24_168_test_1, lag=168, differences = 1)
library(forecast)
library(tseries)
#creating time series
y <- read.csv('/Users/fuyincherng/Documents/EPFLCourse/2017 spring/PerformanceEvaluation/hw/hw4/workingDay_Energy.csv',header= TRUE)
# save a numeric vector containing 72 monthly observations
# from Jan 2009 to Dec 2014 as a time series object
myts <- ts(y[1:(nrow(y)-28),])
ts_test <- y[(nrow(y)-27):nrow(y),]
plot(myts)
plot(ts_test)
lag24 <- diff(myts, lag=24, differences = 1)
plot(lag24)
lag24_168 <- diff(lag24, lag=168, differences = 1)
plot(lag24_168)
lag24_168_test_1 <- diff(ts_test, lag=24, differences = 1)
lag24_168_test <- diff(lag24_168_test_1, lag=168, differences = 1)
plot(lag24_168_test)
lag24_168_test_1 <- diff(ts_test, lag=24, differences = 1)
lag24_168_test <- diff(lag24_168_test_1, lag=168, differences = 1)
plot(lag24_168_test)
?plot
myts <- ts(y[1:(nrow(y)-168),])
ts_test <- y[(nrow(y)-167):nrow(y),]
plot(myts)
plot(ts_test)
# make difference
lag24 <- diff(myts, lag=24, differences = 1)
lag24_168 <- diff(lag24, lag=168, differences = 1)
plot(lag24_168)
lag24_168_test_1 <- diff(ts_test, lag=24, differences = 1)
lag24_168_test <- diff(lag24_168_test_1, lag=168, differences = 1)
plot(lag24_168_test)
myts <- ts(y[1:(nrow(y)-500),])
ts_test <- y[(nrow(y)-499):nrow(y),]
plot(myts)
plot(ts_test)
# make difference
lag24 <- diff(myts, lag=24, differences = 1)
lag24_168 <- diff(lag24, lag=168, differences = 1)
plot(lag24_168)
lag24_168_test_1 <- diff(ts_test, lag=24, differences = 1)
lag24_168_test <- diff(lag24_168_test_1, lag=168, differences = 1)
plot(lag24_168_test)
adf.test(myts);
adf.test(lag24_168);
acf(myts)
pacf(myts)
acf(lag24_168)
pacf(lag24_168)
fit_2 <- arima(lag24_168,c(1,0,0))
accuracy(fit_2)
fit_3 <- arima(lag24_168,c(2,0,0))
accuracy(fit_3)
AIC(fit_2)
AIC(fit_3)
Acf(residuals(fit_2))
Acf(residuals(fit_3))
predic_2 <- forecast(fit_2, 500)
predic_3 <- forecast(fit_3, 500)
result_2 <- predic_2$mean
result_3 <- predic_3$mean
plot(result_3)
lines(lag24_168_test)
plot(result_2)
result_2
summary(result_2)
predic_2
predic_2$level
predic_2$series
predic_2$lower
predic_2 <- forecast(fit_2, 2)
result_2 <- predic_2$mean
plot(result_2)
plot(forecast(fir_2,500))
plot(forecast(fit_2,500))
residuals(fit_2)
plot(residuals(fit_2))
residuals(fit_3)
Acf(residuals(fit_3))
predic_2 <- forecast(fit_2, 3)
plot(forecast(fit_2, 3))
result_2 <- predic_2$mean
plot(result_2)
lines(lag24_168_test[1:3,])
lag24_168_test[1:3,]
lag24_168_test[1:3]
plot(lag24_168_test[1:3])
lag24_168
lag24_168[1:(nrow(lag24_168)-24)]
lag24_168[1]
lag24_168[1:5]
nrow(lag24_168)
fit <- lag24_168[1:3700]
test <- lag24_168[3700:2795]
myts <- ts(fit)
fit_ts <- ts(fit)
fit_3 <- arima(fit_ts,c(2,0,0))
accuracy(fit_3)
predic_3 <- forecast(fit_3, 95)
result_3 <- predic_3$mean
summary(result_3)
plot(result_3)
plot(test)
plot(test)
line(result_3)
vline(result_3)
line(result_3)
plot(result_3)
plot(test)
plot(cars)
print("this is test")
read.csv("/Users/fuyincherng/Documents/EPFLCourse/semester project/JobAdsAnalysis/JobAdAll_2017.csv")
data <- read.csv("/Users/fuyincherng/Documents/EPFLCourse/semester project/JobAdsAnalysis/JobAdAll_2017.csv")
View(data)
library(rjson)
stck <- fromJSON(file = "/Users/fuyincherng/Documents/jobs-multi-master/result/Stackoverflow/Stackoverflow__MayJune.json")
stck[[1]]$Description
stck[[1]]$Description$Responsibilities
stck[[1]]$Description$Skills
install.packages("shiny")
library(shiny)
library(shiny)
runExample("01_hello")
?fluidPage
runApp("/Users/fuyincherng/Documents/EPFLCourse/2017spring/Computational Social Science/Visualization")
library(shiny)
runApp("/Users/fuyincherng/Documents/EPFLCourse/2017spring/Computational Social Science/Visualization")
runApp("/Users/fuyincherng/Documents/EPFLCourse/2017spring/Computational Social Science/Visualization")
runApp('Documents/EPFLCourse/2017spring/Computational Social Science/Visualization')
runApp('Documents/EPFLCourse/2017spring/Computational Social Science/Visualization')
runApp('Documents/EPFLCourse/2017spring/Computational Social Science/Visualization')
runApp('Documents/EPFLCourse/2017spring/Computational Social Science/JobAds_Vis')
runApp('Documents/EPFLCourse/2017spring/Computational Social Science/JobAds_Vis')
runApp('Documents/EPFLCourse/2017spring/Computational Social Science/JobAds_Vis')
runApp('Documents/EPFLCourse/2017spring/Computational Social Science/JobAds_Vis')
runApp('Documents/EPFLCourse/2017spring/Computational Social Science/JobAds_Vis')
runApp('Documents/EPFLCourse/2017spring/Computational Social Science/JobAds_Vis')
runApp('Documents/EPFLCourse/2017spring/Computational Social Science/JobAds_Vis')
runApp('Documents/EPFLCourse/2017spring/Computational Social Science/JobAds_Vis')
runApp('Documents/EPFLCourse/2017spring/Computational Social Science/JobAds_Vis')
?selectInput
runApp('Documents/EPFLCourse/2017spring/Computational Social Science/JobAds_Vis')
runApp('Documents/EPFLCourse/2017spring/Computational Social Science/JobAds_Vis')
runApp('Documents/EPFLCourse/2017spring/Computational Social Science/JobAds_Vis')
runApp('Documents/EPFLCourse/2017spring/Computational Social Science/JobAds_Vis')
runApp('Documents/EPFLCourse/2017spring/Computational Social Science/JobAds_Vis')
runApp('Documents/EPFLCourse/2017spring/Computational Social Science/JobAds_Vis')
runApp('Documents/EPFLCourse/2017spring/Computational Social Science/JobAds_Vis')
runApp('Documents/EPFLCourse/2017spring/Computational Social Science/JobAds_Vis')
runApp('Documents/EPFLCourse/2017spring/Computational Social Science/JobAds_Vis')
runApp('Documents/EPFLCourse/2017spring/Computational Social Science/JobAds_Vis')
runApp('Documents/EPFLCourse/2017spring/Computational Social Science/JobAds_Vis', display.mode = "showcase")
couties <- readRDS("Data/conties.rds")
couties <- readRDS("/Users/fuyincherng/Documents/EPFLCourse/2017spring/Computational Social Science/JobAds_Vis/Data/counties.rds")
head(counties)
head(couties)
View(couties)
install.packages(c("maps", "mapproj"))
library(maps)
library(mapproj)
source("/Users/fuyincherng/Documents/EPFLCourse/2017spring/Computational Social Science/JobAds_Vis/helpers.R")
percent_map(couties$white, "darkgreen", "% White")
source("helpers.R")
shiny::runApp('Documents/EPFLCourse/stockVis')
library(rjson)
setwd('/Users/fuyincherng/Documents/jobs-multi-master/result/')
filenum <- 734#7155 # total number of file to merge
jall <- list()
for(F in 1:filenum){
j1 <- fromJSON(file = paste0("Dice_9_2017/Dice_job_p",F,".json"))
jall <- c(j1, jall)
}
exportJson <- toJSON(jall)
write(exportJson,"Dice_job_all_9_2017.json")
json_data <- fromJSON(file="Dice_job_all_9_2017.json")
#j1[[1]]$Query: the first job's query in j1
#convert to csv: job title, category, dates, url
Title <- list()
Category <- list()
Date <- list()
Url <- list()
Location <- list()
for(i in 1:length(json_data)){ #length(json_data)
Title[i] <- json_data[[i]]$Title
Category[i] <- json_data[[i]]$OccupationalCategory
Date[i] <- json_data[[i]]$DatePosted
Url[i] <- json_data[[i]]$Url
Location[i] <- json_data[[i]]$Location
}
Title <- unlist(Title)
Category <- unlist(Category)
Date <- unlist(Date)
Url <- unlist(Url)
Location <- unlist(Location)
job_csv <- cbind(Title,Category,Date,Location,Url)
write.csv(job_csv,file ="Dice_job_all_9_2017.csv")
setwd('/Users/fuyincherng/Documents/jobs-multi-master/result')
jobs <- read.csv('Dice_job_all_9_2017.csv', stringsAsFactors = F)
#write.csv(jobs_2017$Url,file='Adzuna_2017_url.csv',row.names = F)
write(jobs$Url,file="Dice_job_all_9_2017_url.txt")
setwd('/Users/fuyincherng/Documents/jobs-multi-master/result')
jobs <- read.csv('Dice_job_all_9_2017.csv', stringsAsFactors = F)
#write.csv(jobs_2017$Url,file='Adzuna_2017_url.csv',row.names = F)
write(jobs$Url,file="Dice_job_all_9_2017_url.txt")
